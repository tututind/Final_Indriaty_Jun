---
title: "Midterm: Suggestion for Home Price Prediction Modeling in Philadelphia"
author: "Agarwal, Vrinda; Jun, Youngsang"
date: "October 11, 2024 / November 12, 2024 (resubmitted)"
output: 
  html_document: 
    theme: readable
    code_folding: hide
editor_options: 
  markdown: 
    wrap: sentence
---

# Introduction

Zillow has struggled to accurately estimate home prices, partly because it is a national platform that often lacks detailed, localized information.
**Urban dynamics have become increasingly complex, and understanding housing markets requires a more granular approach that incorporates local factors like government policies, crime, gentrification, pollution levels, and access to public transit.** Our objective is to use local data to create a model that predicts Philadelphia home prices with greater accuracy.

This markdown consists of two parts.
First, we analyze the current house prices prediction model.
We determine the correlation and spatial autocorrelation between house prices and variables in the current model, and derive the problems of the house prices prediction model.
Second, we propose an improved model.
After adding six continuous and categorical variables that affect house prices but are not considered in the current model to the model, we will predict house prices and verify the accuracy and generalization of the improved model.

Our overall modeling strategy focuses on using open-source datasets, such as those available from the Philadelphia Open Data Portal, to enhance our understanding of the local housing market.
**By incorporating data on crime, zoning, pollution, and transit-oriented development, we aim to build a more contextually rich model that reflects the true dynamics of the city.**

# Methods

Our approach to developing a more accurate home price prediction model for Philadelphia involved three main phases: data collection and preparation, analysis of the existing OLS model, and proposal of an improved model.

1.⁠ ⁠Data Collection and Preparation We gathered localized datasets from the Philadelphia Open Data Portal, aiming to capture the unique characteristics influencing home prices within the city.
These datasets included:

-   Property Attributes: sale price, total area, livable area, age, and internal/external quality assessments.
    Neighborhood Characteristics: crime rates, public amenities, and zoning classifications.

-   Accessibility Indicators: transit-oriented development zones, proximity to subway stations, and public parks.

-   Socioeconomic Indicators: demographic trends and school quality.
    To ensure consistent spatial alignment, we mapped all spatial datasets to Philadelphia’s local coordinate system (ESRI:102728).
    Outliers were identified and removed, such as properties with sale prices exceeding \$5 million, to focus the analysis on a more representative price range.

2.⁠ ⁠Analysis of Current OLS Model

We analyzed Zillow’s existing OLS model by examining correlations between sale price and both continuous (e.g., age, total area) and categorical variables (e.g., quality grade, view type).
Our analysis showed:

Negative correlation between age and sale price, suggesting that older properties tend to sell for less.

Positive correlation between total area, livable area, and sale price, indicating larger properties command higher prices.

We evaluated these correlations using a correlation matrix, allowing us to select non-collinear predictors for regression.
In this phase, we tested multiple regression models using variables like total livable area, number of bathrooms, and internal/external condition, observing that livable area was the most significant predictor.

3.⁠ ⁠Proposed Model

We sought to improve prediction accuracy by integrating six new variables beyond the original model:

-   Crime Data: Frequency of incidents per neighborhood to capture safety perception.

-   Shooting Incidents: High-density crime areas, often perceived as riskier.
    Public Transit Proximity: Impact of TOD zones on home values.

-   School Access: Number of schools per census tract as an indicator of educational access.

-   Environmental Quality: Pollution and green space proximity.

-   Gentrification Indicators: Presence of rapidly changing areas.

Using multiple linear regression, we compared our model’s Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) against Zillow’s baseline.
We implemented 100-fold cross-validation to assess model generalizability, allowing systematic testing across varied training and testing splits.
We also applied spatial analysis techniques, such as Moran’s I, to evaluate spatial clustering in model errors, highlighting areas for improvement.

4.⁠ ⁠Statistical and Spatial Analysis

We assessed spatial lags for both price and model errors to understand the clustering of predictions and identify areas where neighborhood effects were inadequately captured.
Additionally, we tested spatial autocorrelation in errors using Moran’s I, finding positive autocorrelation that suggested the model could benefit from further integration of neighborhood-specific factors.

This approach leverages spatial and socioeconomic variables to capture Philadelphia’s local housing dynamics, ultimately reducing MAE and MAPE compared to Zillow’s model.
Further refinements, such as geographically weighted regression and enhanced socio-demographic data, could improve predictive accuracy across diverse neighborhoods and price ranges.

# 1. Data Collection and Preparation

a.  Load Libraries and Initial Setup

```{r setup, warning = FALSE, message = FALSE, results = "hide"}
# Install and Load Libraries
# install.packages('mapview')
library(tidyverse)
library(tidycensus)
library(sf)
library(knitr)
library(kableExtra)
library(mapview)
library(dplyr)
library(scales)
library(viridis)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot)
library(jtools)     # for regression model plots
library(broom)
library(tufte)
library(rmarkdown)
library(pander)
library(classInt)
library(ggplot2)
library(units)

# functions and data directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

# Source for Multiple Ring Buffer
source(
  paste0("https://raw.githubusercontent.com/urbanSpatial/",
         "Public-Policy-Analytics-Landing/master/functions.r"))

# Set Color Table
palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")

# Disable Scientific Notation
options(scipen=999)
options(tigris_class = "sf")

# Set ACS API Key
census_api_key("b2835f54d89a4499ba29829c908967b86765b345", overwrite = TRUE)
```

b.  Data loading

We used spatial data techniques to ensure all data were correctly mapped to Philadelphia’s local coordinate system (ESRI:102728).

```{r read_data, warning = FALSE, message = FALSE, results = "hide"}
# Load Philadelphia Zillow Data
philly.sf <- st_read("~/Documents/Public Policy/Midterm_Agarwal_Jun/Data/studentData.geojson") %>% 
#  as.data.frame() %>%
  st_as_sf(crs = 4326)%>%
  st_transform(crs = 'ESRI:102728') 

# Get ACS data
nhoods <-  
  get_acs(geography = "tract",
          variables = c("B01003_001E","B02001_002E",
                        "B06011_001E"), 
          year=2020, state="PA",
          county="Philadelphia", geometry=TRUE) %>%
  st_transform(crs = 'ESRI:102728') %>%
  separate(NAME, into = c("Census_Tract", "City_State"), sep = ", ", extra = "merge") %>%
  mutate(
    Census_Tract = gsub("Census Tract ", "", Census_Tract),
    City_State = gsub(" County, Pennsylvania", "", City_State)) %>%
  dplyr::select(  -moe)%>%
  spread(key = variable, value = estimate) %>%
  rename(TotalPop = B01003_001,
         NumberWhites = B02001_002,
         Median_Income = B06011_001) %>%
  mutate(percentWhite = NumberWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(Median_Income > 35322, "High Income", "Low Income"))

neighborhoods <- 
  st_read("https://raw.githubusercontent.com/blackmad/neighborhoods/master/philadelphia.geojson") %>%
  st_transform(crs = 'ESRI:102728') 

# Filter to eliminate outlier
philly.sf_filtered <- philly.sf %>%
  filter(sale_price < 5000000 & toPredict == "MODELLING") %>%
  mutate(pricepersq = ifelse(total_area > 10, sale_price / total_area, 0)) 
philly.sf_filtered <- philly.sf_filtered[!is.na(as.numeric(philly.sf_filtered$pricepersq)), ]
```

## House Price per square foot in Philadelphia

The following chart shows House Price per square foot in Philadelphia.
Center City and the Northeast are mostly in the 1st quintile for house prices per square foot, while Southwest, Northwest, and Northeast Philadelphia are around in the 3rd quintile.
By mapping these prices, we can pinpoint high-demand neighborhoods and observe how features like proximity to parks or specific zoning affect property desirability.
In addition, it helps us understand the spatial distribution of property values and reveals which areas are more desirable based on pricing trends.

```{r price_per_sqft, warning = FALSE, message = FALSE}
ggplot() + 
  geom_sf(data=nhoods, fill="#DDD", color="white")+ 
  geom_sf(data = philly.sf_filtered, aes(color = q5(pricepersq)),  
          show.legend = "point", size = 0.5, alpha = 1) +  
  scale_color_manual(values = palette5,
                     labels=qBr(philly.sf_filtered, "pricepersq"),
                     name="Quintile\nBreaks($/sqft)") + 
  labs(
    title = "House Price per square foot in Philadelphia",
    subtitle = "Assessment date: May 24, 2022-August 14, 2023",
    caption = "Data: U.S. Census Bureau, Zillow") +
  theme_void()
#mapview(philly.sf_filtered)
```

## Summary of Other Variables

```{r summarystats, warning = FALSE, message = FALSE}
# Drop geometry and calculate summary statistics
data_for_summary <- philly.sf_filtered %>%
  st_drop_geometry() %>%
  select(
    number_of_bathrooms,
    number_of_bedrooms,
    total_area,
    total_livable_area,
    year_built,
    exterior_condition,
    interior_condition,
    sale_price
  ) %>%
  mutate(
    property_age = 2024 - year_built # Assuming '2024' as the current year for age
  )

# Calculate the summary statistics
sum_stats_combined <- data_for_summary %>%
  summarize(
    Mean = round(c(
      mean(number_of_bathrooms, na.rm = TRUE),
      mean(number_of_bedrooms, na.rm = TRUE),
      mean(total_area, na.rm = TRUE),
      mean(total_livable_area, na.rm = TRUE),
      mean(property_age, na.rm = TRUE),
      mean(exterior_condition, na.rm = TRUE),
      mean(interior_condition, na.rm = TRUE),
      mean(sale_price, na.rm = TRUE)), 2),
    
    Median = round(c(
      median(number_of_bathrooms, na.rm = TRUE),
      median(number_of_bedrooms, na.rm = TRUE),
      median(total_area, na.rm = TRUE),
      median(total_livable_area, na.rm = TRUE),
      median(property_age, na.rm = TRUE),
      median(exterior_condition, na.rm = TRUE),
      median(interior_condition, na.rm = TRUE),
      median(sale_price, na.rm = TRUE)), 2),
    
    Min = round(c(
      min(number_of_bathrooms, na.rm = TRUE),
      min(number_of_bedrooms, na.rm = TRUE),
      min(total_area, na.rm = TRUE),
      min(total_livable_area, na.rm = TRUE),
      min(property_age, na.rm = TRUE),
      min(exterior_condition, na.rm = TRUE),
      min(interior_condition, na.rm = TRUE),
      min(sale_price, na.rm = TRUE)), 2),
    
    Max = round(c(
      max(number_of_bathrooms, na.rm = TRUE),
      max(number_of_bedrooms, na.rm = TRUE),
      max(total_area, na.rm = TRUE),
      max(total_livable_area, na.rm = TRUE),
      max(property_age, na.rm = TRUE),
      max(exterior_condition, na.rm = TRUE),
      max(interior_condition, na.rm = TRUE),
      max(sale_price, na.rm = TRUE)), 2),
    
    SD = round(c(
      sd(number_of_bathrooms, na.rm = TRUE),
      sd(number_of_bedrooms, na.rm = TRUE),
      sd(total_area, na.rm = TRUE),
      sd(total_livable_area, na.rm = TRUE),
      sd(property_age, na.rm = TRUE),
      sd(exterior_condition, na.rm = TRUE),
      sd(interior_condition, na.rm = TRUE),
      sd(sale_price, na.rm = TRUE)), 2)
  ) %>%
  mutate(
    `Variable Name` = c("number_of_bathrooms", "number_of_bedrooms", 
                        "total_area", "total_livable_area", 
                        "property_age", "exterior_condition", 
                        "interior_condition", "sale_price"),
    
    Description = c("Number of Bathrooms", "Number of Bedrooms", 
                    "Total Area of Property", "Total Livable Area of Property",
                    "Property Age", "Exterior Condition Rating", 
                    "Interior Condition Rating", "Sale Price of Property"),
    
    Category = c(rep("Physical characteristics", 4), 
                 "Age", rep("Condition", 2), "Financial")
  ) %>%
  select(Category, `Variable Name`, Description, everything())

# Display the summary statistics table with formatting
sum_stats_combined %>%
  kable(format = "html", caption = "Table: Summary statistics for numeric variables by category") %>%
  row_spec(1:4, background = "#ffffff50") %>%
  row_spec(5, background = "#f2f2f250") %>%
  row_spec(6:7, background = "#e6e6e680") %>%
  kable_styling(bootstrap_options = c("hover"), fixed_thead = TRUE)
```

# 2. Current OLS Model Analysis

## Price as a function of continuous variables

We looked at the relationship between House Price i.e. `sale_price` and `Age`, `total_area`, and `total_livable_area`, which are continuous variables currently used by Zillow.
`Age` was calculated by subtracting the year of completion, `year_built`, from this year, 2024.
As a result, `Age` and `sale_price` showed a negative correlation, while `total_area` and `sale_price` and `total_livable_area` and `sale_price` showed a positive correlation.
We filtered our datasets to remove outliers and errors, such as excluding properties with sale prices above \$5 million to focus on the majority market segment.

```{r current_continuous, warning = FALSE, message = FALSE}
# Price as a function of continuous variables
philly.sf_filtered %>%
  st_drop_geometry() %>%
  mutate(Age = 2024 - year_built) %>%
  dplyr::select(sale_price, total_area, Age, total_livable_area) %>%
  filter(sale_price <= 5000000, total_area <= 10000, total_livable_area <= 5000, 
         Age < 500) %>% # Filter out crazy outliers
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + 
     geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, ncol = 3, scales = "free") +
     labs(title = "Price as a function of continuous variables",
          subtitle="Sale Price: $, Age: year(s), Area: sqft, Total Livable Area: sqft") +
     theme_minimal()
```

## Price as a function of categorical variables

We also examine the relationship between `sale_price` and `quality_grade`, `view_type`, `exterior_condition`, and `interior_condition`, which are categorical variables currently used by Zillow.
`Quality_grade` has two types of grading systems, so we changed the 1-6 system to an S-A-B-C-D-E system.
The following histograms show that as the condition or quality grade goes high, the sale price tends to increase.

```{r current_categorical, warning = FALSE, message = FALSE}
# Price as a function of categorical variables

philly.sf_filtered <- philly.sf_filtered %>%
  mutate(
    quality_grade_new = case_when(
      quality_grade == "1 " ~ "S ",  
      quality_grade == "2 " ~ "A ",  
      quality_grade == "3 " ~ "B ",  
      quality_grade == "4 " ~ "C ", 
      quality_grade == "5 " ~ "D ", 
      quality_grade == "6 " ~ "E ", 
      TRUE ~ as.character(quality_grade)        
    )
  )

philly.sf_filtered$quality_grade_new <- factor(philly.sf_filtered$quality_grade_new, levels = c("S+", "S ", "A+", "A ", "A-", "B+", "B ", "B-", "C+", "C ", "C-", "D+", "D ", "D-", "E+", "E ", "E-"))
# levels(philly.sf_filtered$quality_grade_new)
philly.sf_filtered %>%
  st_drop_geometry() %>%
  dplyr::select(sale_price, quality_grade_new, view_type, exterior_condition, interior_condition) %>%
  filter(sale_price <= 1000000, !is.na(interior_condition), interior_condition != 0, view_type != "", view_type != 0, quality_grade_new != "") %>% # Filter out crazy outliers
  gather(Variable, Value, -sale_price) %>%
  ggplot(aes(Value, sale_price)) +
    geom_bar(position = "dodge", stat="summary", fun="mean")+
    facet_wrap(~Variable, ncol = 2, scales = "free") +
    labs(title = "Price as a function of categorical variables", y="Mean_Price") +
    plotTheme() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))+
    theme_minimal()
```

The following diagram shows correlation matrix across numeric variables, which helps us decide to choose which variable for regression.
It shows how different property and neighborhood numeric factors relate to each other and to home prices.
It uses values between -1 and 1 to indicate the strength and direction of these relationships: - Positive values mean that as one variable increases, so does the other (e.g., property size and price).
- Negative values show that as one variable increases, the other decreases (e.g., distance from the city center and price).
- Values near zero suggest no clear relationship.

Since it is helpful for analysis to perform regression by selecting variables that are not correlated with each other, `year_built`, `total_area`, and `number_stories` were chosen for the subsequent regression analysis.

```{r current_correlation, warning = FALSE, message = FALSE}
numericVars <- 
  select_if(st_drop_geometry(philly.sf_filtered), is.numeric) %>% na.omit()

ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation across numeric variables") 
```

## Multiple Linear Regression Model Results

The following is the result of linear regression analysis of `sale_price` and `total_livable_area`.
If this result is statistically significant, we can predict `sale_price` from `total_livable_area` using the formula SalePrice_i = -65,358 + 252.99Xi + ei.
The example below is statistically significant because the p-value is small, and if `total_livable_area` are 2,262 sqft, we can say that the predicted `sale_price` is -65,358+252.99x2,262=\$507,000.

```{r current_regression, warning = FALSE, message = FALSE}
livingReg <- lm(sale_price ~ total_livable_area, data = philly.sf_filtered)
summary(livingReg)
```

This regression analysis can be extended to multiple variables.
The following denotes that `sale_price`, `exterior_condition`, `interior_condition`, `number_of_bathrooms`, `number_of_bedrooms`, `total_livable_area`, `type_heater`, `year_built`, and `number_stories` were found to be correlated with `sale price`, while `total_area` and `view_type` showed no clear evidence of correlation with `sale price`.
We removed `parcel_shape` because it contains only one ‘D’ value, which makes dividing the data into test and training sets impossible.

```{r current_regression1, warning = FALSE, message = FALSE}
reg1 = lm(sale_price ~ ., data = 
            st_drop_geometry(philly.sf_filtered) %>%
            filter(number_of_bedrooms <13) %>%
            dplyr::select(sale_price, exterior_condition, interior_condition, 
                          number_of_bathrooms, number_of_bedrooms,
                          total_area, total_livable_area, type_heater, 
                          view_type, year_built, number_stories))
summary(reg1)
```

We divided `number_stories` into three groups (Up to 2 Floors, 3 Floors, 4+ Floors) and performed regression analysis.
The results are as follows.

```{r current_number_stories, warning = FALSE, message = FALSE, results='hide'}
# Categorize Number of Stories
philly.sf_filtered <-
  philly.sf_filtered %>%
  mutate(number_stories.cat = case_when(
    number_stories >= 0 & number_stories < 3 ~ "Up to 2 Floors",
    number_stories >= 3 & number_stories < 4 ~ "3 Floors",
    number_stories > 4 ~ "4+ Floors"
  ))
```

```{r current_regression2, warning = FALSE, message = FALSE}
reg2 <- lm(sale_price ~ ., data = 
            st_drop_geometry(philly.sf_filtered) %>%
            dplyr::select(sale_price, exterior_condition, interior_condition, 
                          number_of_bathrooms, number_of_bedrooms,
                          total_area, total_livable_area, type_heater, 
                          view_type, year_built, number_stories.cat))
summary(reg2)
```

## Accuracy of the Current Model

To assess the accuracy of the current model, we calculated the Mean Absolute Error (MAE) and Mean Absolute Percent Error (MAPE).
To do this, the dataset was randomly divided into training data and testing data in a ratio of 6:4, and then the difference between the observed value and the actual value was calculated.

The MAE is the average of the absolute differences between predicted and observed values, while the MAPE is the average of the absolute percentage differences between predicted and observed values.
The MAE and MAPE are useful metrics for evaluating the accuracy of regression models, as they provide a measure of how well the model predicts actual values.

```{r current_MAE1, warning = FALSE, message = FALSE}
inTrain <- createDataPartition(
              y = paste(philly.sf_filtered$number_stories.cat, 
                        philly.sf_filtered$view_type, philly.sf_filtered$heat_type), 
              p = .60, list = FALSE)
philly.training <- philly.sf_filtered[inTrain,] 
philly.test <- philly.sf_filtered[-inTrain,]  
```

```{r current_MAE2, warning = FALSE, message = FALSE}
reg.training <- 
  lm(sale_price ~ ., data = as.data.frame(philly.training) %>% 
                             dplyr::select(sale_price, exterior_condition, interior_condition, number_of_bathrooms, number_of_bedrooms, total_area, total_livable_area, type_heater, view_type, year_built, number_stories.cat))

summary(reg.training)

```

```{r current_MAE3, warning = FALSE, message = FALSE}
philly.test <-
  philly.test %>%
  mutate(Regression = "Baseline Regression",
         sale_price.Predict = predict(reg.training, philly.test),
         sale_price.Error = sale_price.Predict - sale_price,
         sale_price.AbsError = abs(sale_price.Predict - sale_price),
         sale_price.APE = (abs(sale_price.Predict - sale_price)) / sale_price) %>%
  filter(sale_price < 5000000) 

ggplot()+
  geom_histogram(data = philly.test, aes(x = sale_price.AbsError), binwidth = 10000, fill = "orange") +
  scale_x_continuous(breaks=seq(0, 1000000, by = 100000), )+
  coord_cartesian(xlim = c(0, 1000000)) +
  labs(title = "Distribution of Prediction Errors",
  subtitle = "",
  caption = "") +
  theme_minimal()
```

The result of the current model is as follows:\
- Mean Absolute Error (MAE): \$100,278.2\
- Mean Absolute Percent Error (MAPE): 63.5%\
\* The result of knit can be different.

The error is not trivial given the mean `sale_price` of \$271,929.
This suggests that the current model may not be accurately capturing the underlying patterns in the data.

```{r current_MAE4, warning = FALSE, message = FALSE}
#Mean Absolute Error (MAE)
mean(philly.test$sale_price.AbsError, na.rm = T)
```

```{r current_MAE5, warning = FALSE, message = FALSE}
# Mean Absolute Percent Error (MAPE)
mean(philly.test$sale_price.APE, na.rm = T)
```

The following plot shows that the model and perfect prediction.
The orange line represents a perfect prediction (y=x), and the green line represents average prediction.
This plot shows that predicted sale price was a little over estimated.

```{r current_MAE6, warning = FALSE, message = FALSE}
ggplot(data=philly.test, aes(sale_price.Predict, sale_price)) +
     geom_point(size = .5) + 
     geom_abline(intercept = 0, slope = 1, color = "#FA7800") +
     geom_smooth(method = "lm", se=F, colour = "#25CB10") +
    coord_cartesian(xlim = c(0, 4000000), ylim = c(0, 4000000)) +
     labs(title = "Price as a function of continuous variables",
          subtitle = "Orange line represents a perfect prrediction (y=x)\nGreen line represents average prediction") +
     theme_minimal()
```

## Generalizability - 'k-fold' Cross-Validation

To assess the generalizability of the current model, we performed k-fold cross-validation with k=100.
This algorithm involves splitting the dataset into 100 subsets, training the model on 99 subsets, and testing it on the remaining subset.
This process is repeated 100 times, with each subset used as the test set once.
The results are then averaged to provide an estimate of the model's performance on new data.

```{r current_cv1, warning = FALSE, message = FALSE}
fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(sale_price ~ ., data = st_drop_geometry(philly.sf_filtered) %>% 
                                dplyr::select(sale_price, exterior_condition,                
                                           interior_condition, 
                                           number_of_bathrooms, number_of_bedrooms,
                                           total_area, total_livable_area, type_heater, 
                                           view_type, year_built, 
                                           number_stories.cat), 
     method = "lm", trControl = fitControl, na.action = na.pass)

reg.cv

```

The distribution of MAE values is shown in the histogram below.

```{r current_cv2, warning = FALSE, message = FALSE}
ggplot()+
  geom_histogram(data = reg.cv$resample, aes(x = MAE), binwidth = 10000, fill = "orange") +
  scale_x_continuous(breaks=seq(0, 600000, by = 100000), )+
  coord_cartesian(xlim = c(0, 600000)) +
  labs(title = "Distribution of MAE",
  subtitle = "k-fold cross-validation; k=100",
  caption = "") +
  theme_minimal()
```

The first plot shows the predicted sale price in the test set(`philly.test`), and the second plot shows the absolute sale price errors.
No matter how accurate this model may be, we can see that some of the errors still remain clustered on the second map.
It suggests that other factors with spatial correlations remain unmodeled.
We will explore this further in the next step by applying spatial lag analysis and Moran's I.

```{r current_cv3, warning = FALSE, message = FALSE}
philly.test <- philly.test[!is.na(philly.test$sale_price.Predict),]
ggplot() + 
  geom_sf(data=nhoods, fill="#DDD", color="white")+ 
  geom_sf(data = philly.test, aes(color = q5(sale_price.Predict)),  
          show.legend = "point", size = 0.5, alpha = 1) +  
  scale_color_manual(values = palette5,
                     labels=qBr(philly.test, "sale_price.Predict"),
                     name="Quintile\nBreaks($/sqft)") + 
  labs(
    title = "Predicted Sale Price in Philadelphia",
    subtitle = "Assessment date: May 24, 2022-August 14, 2023",
    caption = "Data: U.S. Census Bureau, Zillow") +
  theme_void()
```

```{r current_cv4, warning = FALSE, message = FALSE}
ggplot() + 
  geom_sf(data=nhoods, fill="#DDD", color="white")+ 
  geom_sf(data = philly.test, aes(color = q5(sale_price.AbsError)),  
          show.legend = "point", size = 0.5, alpha = 1) +  
  scale_color_manual(values = palette5,
                     labels=qBr(philly.test, "sale_price.AbsError"),
                     name="Quintile\nBreaks($/sqft)") + 
  labs(
    title = "Test Set Absolute Sale Price Errors in Philadelphia",
    subtitle = "Assessment date: May 24, 2022-August 14, 2023",
    caption = "Data: U.S. Census Bureau, Zillow") +
  theme_void()
```

## Check if Prices and Errors cluster

### (1) Spatial Lags

In this step, we calculated for each home sale, the average sale price of its k=5 nearest neighbors.
This spatial lag variable, `lagPrice`, represents the average sale price of the five nearest neighbors of each home sale.
We then used this variable to assess the spatial autocorrelation of sale prices and errors.

```{r current_spatial_lags1, warning = FALSE, message = FALSE}
coords <- st_coordinates(philly.sf_filtered) 

neighborList <- knn2nb(knearneigh(coords, 5))

spatialWeights <- nb2listw(neighborList, style="W")

philly.sf_filtered$lagPrice <- lag.listw(spatialWeights, philly.sf_filtered$sale_price)

```

```{r current_spatial_lags2, warning = FALSE, message = FALSE}
philly.test <- philly.test[!is.na(philly.test$sale_price.Error),]
coords.test <-  st_coordinates(philly.test) 
neighborList.test <- knn2nb(knearneigh(coords.test, 5))
spatialWeights.test <- nb2listw(neighborList.test, style="W") 
```

There is still positive correlation between `sale_price` and `lagPrice` and between `sale_price.Error` and `lagPriceError`.
This suggests that the model's errors are clustered spatially, indicating that the model is not adequately capturing localized factors.

```{r current_spatial_lags3, warning = FALSE, message = FALSE}
# Sale Price as a function of the Spatial Lag of Price
philly.test %>% 
  mutate(lagPrice = lag.listw(spatialWeights.test, sale_price)) %>% 
  ggplot(aes(lagPrice, sale_price))+
  geom_point(color = "orange") +
  geom_smooth(method = "lm", se=F, colour = "#25CB10") +
  labs(title = "Price as a function of the Spatial Lag of Price",
       x = "Spatial Lag of Price (mean price of 5 nearest neighbors)",
       y = "Sale Price") +
  theme_minimal()
```

```{r current_spatial_lags4, warning = FALSE, message = FALSE}
# Error as a Function of the Spatial Lag of Price
philly.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)) %>% 
  ggplot(aes(lagPriceError, sale_price.Error))+
  geom_point(color = "orange") +
  geom_smooth(method = "lm", se=F, colour = "#25CB10") +
  labs(title = "Error as a Function of the Spatial Lag of Price",
       x = "Spatial Lag of Errors (mean error of 5 nearest neighbors)",
       y = "Sale Price Error") +
  theme_minimal()
```

We can calculate the Pearson's R coefficient to test this a different way.

```{r current_spatial_lags5, warning = FALSE, message = FALSE}
pearsons_test <- 
  philly.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error, NAOK=TRUE))

cor.test(pearsons_test$lagPriceError,
         pearsons_test$sale_price.Error, 
         method = "pearson")
```

### (2) Moran's I

By using the Moran's I statistic, we can assess the global degree of clustering or dispersion of sales price values.
The result shows that Moran's I = 0.396, which indicates positive spatial autocorrelation.
It suggests that the model's errors are clustered, often occurring in specific neighborhoods or regions.
If Moran’s I is close to zero, it implies that errors are more randomly distributed, indicating the model might be performing well without significant spatial bias.
To improve this prediction model, we need to find more spatial variables that can reduce the spatial autocorrelation of the errors.

```{r current_moran, warning = FALSE, message = FALSE}

moranTest <- moran.mc(philly.test$sale_price.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "orange",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  theme_minimal()
```

# 3. Proposed OLS Model

## Which Data to be Included

In Chapter 2, we discovered that more spatial variables need to be incorporated on the current Zillow model so that we can reduce the spatial autocorrelation of the errors, and finally improve prediction quality in accuracy and generalization.
Therefore, We collected data from open sources such as the Philadelphia Open Data Portal, focusing on datasets that provide insight into the local character of neighborhoods.

We intentionally excluded factors like walkability because, although important in urban housing preferences, it may not be universally desirable, especially for families who might prefer less densely populated areas.
The following table shows the data types we included and excluded, along with the reasons for our decisions.

```{r proposed_1, warning = FALSE, message = FALSE}

# Create the data frame
data <- data.frame(
  Category = c("Safety & Crime", "Urban Planning & Zoning", "Gentrification Indicators", "Accessibility & Transit", "Environmental Quality", "Education", "Quality of Life", "Public Spaces", "Government & Infrastructure"),
  `Data Type` = c("Crime Data", "Zoning Regulations", "Gentrification Indicators", "Transit Accessibility", "Pollution Levels", "School Quality", "Walkability Index", "Access to Parks", "Government Investments"),
  `Included/Excluded` = c("Included", "Included", "Included", "Included", "Included", "Included", "Excluded", "Excluded", "Excluded"),
  Reason = c(
    "Safety is a critical factor for homeowners.",
    "Impacts property types, uses, and value.",
    "Gentrification trends affect property values.",
    "Commute to work.",
    "Pollution impacts health and desirability.",
    "The quality of local schools is a major consideration for families and directly impacts property values.",
    "Walkability is valued in urban settings but may not be a priority for all buyers, such as families preferring suburban environments.",
    "Access to parks and recreational spaces may have variable impact depending on demographics, making it less reliable for general predictions.",
    "Government resources and investments are often dispersed and long-term, making their direct impact on property values harder to measure."
  )
)

# Create the kable table
kable(data, "html", col.names = c("Category", "Data Type", "Included/Excluded", "Reason")) %>%
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  column_spec(1:4, width = "20em") # Adjust column width for better readability
```

### Summary Statistics Table

This table gives an overview of different features related to properties and their surroundings, grouped into categories like Property Details, Local Amenities, Neighborhood Layout, Crime Data, and Transit/Demographics.

The table breaks down various features of properties and neighborhoods into five main groups: 

- Property Details: How many bedrooms and bathrooms they have, and the year they were built.

- Local Amenities: Features and services that might impact a property’s appeal, such as whether a property has central air or a garage.

- Neighborhood Layout: Zoning rules and the shape of the property lots, giving insight into how the area is organized.

- Crime Data: Frequency of crime incidents in different areas, which can help gauge the safety of a neighborhood.

- Transit/Demographics: Access to public transportation and basic demographic trends like population levels.

```{r proposed_2, warning = FALSE, message = FALSE}

file_path <- "~/Documents/Public Policy/Midterm_Agarwal_Jun/Data/Cleaned_Comprehensive_Summary_Statistics.csv"

summary_stats <- read_csv(file_path)

summary_stats_cleaned <- summary_stats %>%
  drop_na()

summary_stats_cleaned %>%
  kable("html", caption = "Summary Statistics") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

## 6 Newly Included Data and Visualization

We collected 6 additional datasets including crime data in 2018-2023, crime dispatch incidents in 2024, location of subway stations, schools, public parks and recreation areas from the Philadelphia Open Data portal the U.S.
Census Bureau (ACS data) or other data source.
We also cleaned data to exclude records with missing or erroneous geographic points.
We created new variables, such as price per square foot, number of schools per tract, which are crucial for our model.

```{r read_data2, results='hide'}
# Load Philadelphia Zillow Data
philly.sf <- st_read("~/Documents/Public Policy/Midterm_Agarwal_Jun/Data/studentData.geojson") %>% 
#  as.data.frame() %>%
  st_as_sf(crs = 4326)%>%
  st_transform(crs = 'ESRI:102728') 

nhoods <-  
  get_acs(geography = "tract",
          variables = c("B01003_001E","B02001_002E",
                        "B06011_001E"), 
          year=2020, state="PA",
          county="Philadelphia", geometry=TRUE) %>%
  st_transform(crs = 'ESRI:102728') %>%
  separate(NAME, into = c("Census_Tract", "City_State"), sep = ", ", extra = "merge") %>%
  mutate(
    Census_Tract = gsub("Census Tract ", "", Census_Tract),
    City_State = gsub(" County, Pennsylvania", "", City_State)) %>%
  dplyr::select(  -moe)%>%
  spread(key = variable, value = estimate) %>%
  rename(TotalPop = B01003_001,
         NumberWhites = B02001_002,
         Median_Income = B06011_001) %>%
  mutate(percentWhite = NumberWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(Median_Income > 35322, "High Income", "Low Income"))

# Load Police District Data in Philadelphia
philly.police <- st_read("~/Documents/Public Policy/Midterm_Agarwal_Jun/Data/police_districts.geojson") %>% 
#  as.data.frame() %>%
  st_as_sf(crs = 4326)%>%
  st_transform(crs = 'ESRI:102728') 

# Load School Data in Philadelphia
philly.school <- st_read("~/Documents/Public Policy/Midterm_Agarwal_Jun/Data/Schools.geojson") %>% 
#  as.data.frame() %>%
  st_as_sf(crs = 4326)%>%
  st_transform(crs = 'ESRI:102728') 

# Load Green Area (Public Parks Recreation) Data in Philadelphia
philly.PPR <- st_read("~/Documents/Public Policy/Midterm_Agarwal_Jun/Data/PPR_Properties.geojson") %>% 
#  as.data.frame() %>%
  st_as_sf(crs = 4326)%>%
  st_transform(crs = 'ESRI:102728') 

# Load Shooting Incident Data in Philadelphia
philly.shooting <- st_read("~/Documents/Public Policy/Midterm_Agarwal_Jun/Data/shootings.geojson") %>% 
#  as.data.frame() %>%
  st_as_sf(crs = 4326)%>%
  st_transform(crs = 'ESRI:102728') 

ppr_properties_sf <- st_read("~/Documents/Public Policy/Midterm_Agarwal_Jun/Data/PPR_Properties.geojson") %>%
  st_as_sf(crs = 4326)%>%
  st_transform(crs = 'ESRI:102728')

police_districts_sf <- st_read("~/Documents/Public Policy/Midterm_Agarwal_Jun/Data/police_districts.geojson") %>%
  st_as_sf(crs = 4326)%>%
  st_transform(crs = 'ESRI:102728')

shootings_sf <- st_read("~/Documents/Public Policy/Midterm_Agarwal_Jun/Data/shootings.geojson")%>%
  st_as_sf(crs = 4326)%>%
  st_transform(crs = 'ESRI:102728')

# Shooting Incident Data 
shootingPol <- st_join(philly.shooting, nhoods, left = FALSE)
shootingPol_count <- shootingPol %>%
  group_by(Census_Tract) %>%  
  summarise(shooting_count = n())  
nhoods_b <- left_join(nhoods, st_drop_geometry(shootingPol_count), by = "Census_Tract") %>%
  st_sf() %>%
  mutate(shooting_count = ifelse(is.na(shooting_count), 0, shooting_count)) 
  #ggplot()+
  #  geom_sf(data=nhoods_b, aes(fill=shooting_count))

# Green area (Public Parks Recreation) data
PPRPol <- st_join(philly.PPR, nhoods_b, left = FALSE)
PPRPol_count <- PPRPol %>%
  group_by(Census_Tract) %>%  
  summarise(PPR_count = n())  
nhoods_c <- left_join(nhoods_b, st_drop_geometry(PPRPol_count), by = "Census_Tract") %>%
  st_sf() %>%
  mutate(PPR_count = ifelse(is.na(PPR_count), 0, PPR_count)) 
  #ggplot()+
  #  geom_sf(data=nhoods_c, aes(fill=PPR_count))

# Number of school in each tract data
schoolPol <- st_join(philly.school, nhoods_c, left = FALSE)
schoolPol_count <- schoolPol %>%
  group_by(Census_Tract) %>%  
  summarise(school_count = n())  
nhoods_d <- left_join(nhoods_c, st_drop_geometry(schoolPol_count), by = "Census_Tract") %>%
  st_sf() %>%
  mutate(PPR_count = ifelse(is.na(school_count), 0, school_count)) 
  #ggplot()+
  #  geom_sf(data=nhoods_d, aes(fill=school_count))

# Load Crime Data in Philadelphia
philly.crime <- read.csv("~/Documents/Public Policy/Midterm_Agarwal_Jun/Data/CrimeData.csv") 
philly.crime <- subset(philly.crime, the_geom!='0101000020E6100000A5A31CCC262054C0A8BE77C4B61C4540') #error
philly.crime <- subset(philly.crime, the_geom!='0101000020E6100000E3D840DB262054C0CCE8EC09B71C4540') #error
philly.crime <- st_as_sf(philly.crime, coords = c("lng", "lat"), crs = 4326) %>%
  st_transform(crs = 'ESRI:102728') 

# Crime Data 
crimePol <- st_join(philly.crime, nhoods_d, left = FALSE)
crimePol_count <- crimePol %>%
  group_by(Census_Tract) %>%  
  summarise(crime_count = n())  
nhoods_e <- left_join(nhoods_d, st_drop_geometry(crimePol_count), by = "Census_Tract") %>%
  st_sf() %>%
  mutate(crime_count = ifelse(is.na(crime_count), 0, crime_count)) 
  #ggplot()+
  # geom_sf(data=nhoods_e, aes(fill=crime_count))




```

### (1) Number of Public Parks/Recreation Areas by Tract in Philadelphia

The map shows different types of alongside public parks and recreational areas within the city.
Access to green spaces and recreational amenities is a significant driver of housing desirability and value.

```{r proposed_3, warning = FALSE, message = FALSE}

ggplot() + 
  geom_sf(data=st_union(nhoods), fill="#DDD", color="white")+
  geom_sf(data = ppr_properties_sf, fill = "green", color = "darkgreen", shape = 21, size = 1, alpha = 0.8) + # PPR properties
  labs(
    title = "Public Parks/Recreation (PPR) Areas in Philadelphia",
    subtitle = "Visualizing the distribution of PPR properties within city limits",
    caption = "Data: Philadelphia Planning Department, PPR, and City Limits") +
  theme_void() +
  theme(
    legend.position = "right",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )

```

### (2) Police Districts and Population Density in Philadelphia

This map overlays police districts with population density information for each census tract, illustrating how densely populated areas align with police resources.
Population density often correlates with housing demand, as denser areas tend to have higher property values due to limited space.
Understanding how law enforcement resources align with population clusters can also offer insights into perceptions of safety and how these perceptions might impact local housing prices.

```{r proposed_4, warning = FALSE, message = FALSE}
philly_population <- get_acs(geography = "tract",
                             variables = "B01003_001E", # Total population variable
                             year = 2020, state = "PA",
                             county = "Philadelphia", geometry = TRUE) %>%
  st_transform(crs = 'ESRI:102728')

philly_population$area_sq_km <- set_units(st_area(philly_population), "km^2")

# Calculate population density (people per square kilometer) and ensure consistency in units
philly_population$population_density <- philly_population$estimate / drop_units(philly_population$area_sq_km)

# Plot the map showing police districts and population density
ggplot() + 
  geom_sf(data = philly_population, aes(fill = population_density), color = NA) +
  scale_fill_viridis_c(name = "Population Density\n(per sq km)", option = "inferno") +
  geom_sf(data = police_districts_sf, fill = NA, color = "red", size = 0.8) +
  labs(
    title = "Police Districts and Population Density in Philadelphia",
    subtitle = "Population Density per Census Tract with Police District Boundaries",
    caption = "Data: U.S. Census Bureau, Philadelphia Open Data Portal") +
  theme_void()

```

### (3) Density of Shooting Incidents in Philadelphia with Police Districts Overlay

This map illustrates the concentration of shooting incidents and overlays police district boundaries to see how shootings align with law enforcement zones.
Identifying high-risk areas helps us understand which neighborhoods might need more intervention or are perceived as less safe, directly influencing property values.

```{r proposed_5, warning = FALSE, message = FALSE}
# Create the density map with police districts
ggplot() + 
  stat_density2d(data = data.frame(st_coordinates(shootings_sf)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..), 
                 size = 0.01, bins = 40, geom = 'polygon') +
  geom_sf(data = police_districts_sf, fill = NA, color = "black", linetype = "solid", size = 0.5) + # Police districts overlay
  scale_fill_gradient(low = "lightblue", high = "darkblue", 
                      breaks = c(0.000000002, 0.00000002),
                      labels = c("Minimum", "Maximum"),
                      name = "Density") +
  scale_alpha(range = c(0.00, 0.50), guide = "none") +
  labs(title = "Density of Shooting Incidents in Philadelphia",
       subtitle = "Overlaid with Police District Boundaries",
       caption = "Data: Philadelphia Open Data Portal") +
  theme_void()
```

### (4) TOD (public transit) in Philadelphia

This map shows the areas within 0.5 miles (2,641 ft) of the Market-Frankford Line (MFL) and Broad Street Line, which are key public transit routes in Philadelphia.
As we discovered on the last assignment, properties near public transit are often more desirable, as they offer convenient access to transportation and amenities, which can drive up property values.

```{r TOD, warning = FALSE, message = FALSE, result='hide'}
# TOD Data 
MFL <- st_read("https://opendata.arcgis.com/datasets/8c6e2575c8ad46eb887e6bb35825e1a6_0.geojson")
Broad_St <- st_read("https://opendata.arcgis.com/datasets/2e9037fd5bef406488ffe5bb67d21312_0.geojson")

septaStops <- 
  rbind(
     MFL %>% 
      mutate(Line = "MFL") %>%
      dplyr::select(Station, Line),
     Broad_St %>%
      mutate(Line ="Broad_St") %>%
      dplyr::select(Station, Line)) %>%
  st_transform(st_crs(nhoods_e))  

stopBuffer <- st_buffer(septaStops, 2641)
stopUnion <- st_union(st_buffer(septaStops, 2641))
buffer <- stopUnion %>%
      st_sf() 
selectCentroids <-
  st_centroid(nhoods_e)[buffer,] %>%
  st_drop_geometry() %>%
  left_join(., dplyr::select(nhoods_e, GEOID), by = "GEOID") %>%
  st_sf() 

selectCentroids_union <- st_union(selectCentroids)%>%
  st_sf() 

nhoods_f <- 
  rbind(
    st_centroid(nhoods_e)[buffer,] %>%
      st_drop_geometry() %>%
      left_join(nhoods_e) %>%
      st_sf() %>%
      mutate(TOD = "TOD"),
    st_centroid(nhoods_e)[buffer, op = st_disjoint] %>%
      st_drop_geometry() %>%
      left_join(nhoods_e) %>%
      st_sf() %>%
      mutate(TOD = "Non-TOD")) 
```

```{r TOD2, warning = FALSE, message = FALSE}
## Plot TOD
ggplot() + 
  geom_sf(data=nhoods_f, fill="#DDD", color="white")+
  geom_sf(data=septaStops, 
          aes(colour = Line), 
          show.legend = "point", size= 1.5) +
  scale_colour_manual(values = c("orange","blue")) +
  geom_sf(data=selectCentroids_union, fill='transparent', color='red')+
  labs(title = "TOD area in Philadelphia",
    subtitle = "0.5 miles from the MFL and Broad St Line",
    caption = "Data: U.S. Census Bureau, SEPTA") +
  theme_void()
```


### (5) Number of Crime Dispatch by Tract in Philadelphia

This map highlights areas where crime incidents are more frequent, providing a visual representation of crime density across the city.
Safety is a critical factor in homebuyer decisions—people prefer safer neighborhoods, which drives up property values there.
By mapping crime density, we can factor safety into our model, predicting how values might increase in safer areas or decrease in higher-crime zones.

```{r proposed_6, warning = FALSE, message = FALSE}
## Plot Density of Crime Dispatch
ggplot() + 
  geom_sf(data=nhoods, fill="#DDD", color="white")+ 
  stat_density2d(data = data.frame(st_coordinates(philly.crime)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "#25CB10", high = "#FA7800", 
                     breaks=c(0.000000002,0.00000002),
                     labels=c("Minimum", "Maximum"),
                      name = "Density") +
  scale_alpha(range = c(0.00, 0.50), guide = "none") +
  labs(title = "Density of Crime Dispach in Philadelphia",
    subtitle = "Date: January 1, 2024-October 1, 2024",
    caption = "Data: U.S. Census Bureau, Open Data Philly") +
  theme_void()
```

### (6) Number of School by Tract in Philadelphia

This map displays how close properties are to schools, which is a key factor for families seeking homes.
Properties closer to schools often become more desirable, increasing demand.
Families place a premium on properties near quality schools, making school proximity a crucial factor in price determination.

```{r proposed_7, warning = FALSE, message = FALSE}
## Plot School
ggplot() + 
  geom_sf(data=nhoods_f, aes(fill=school_count), color="white")+
  scale_fill_viridis_c(name = "School Count", option = "inferno") +
  labs(title = "Number of Schools by Tract in Philadelphia",
    caption = "Data: U.S. Census Bureau, OpenData Philly") +
  theme_void()
```

## Multiple Linear Regression Model Results

The following plots show the relationship between sale price and various continuous variables.
New data includes crime count, school count, shooting count, and public park and recreation data.
The plots show that sale price is positively correlated with total area, while it is negatively correlated with age, shooting count, public parks and recreation areas and crime count.
The plots also show that sale price is not significantly correlated with school count.

```{r proposed_8, warning = FALSE, message = FALSE}
philly.sf_New <- philly.sf %>%
  filter(sale_price < 5000000 & toPredict == "MODELLING") %>%
  mutate(pricepersq = ifelse(total_area > 10, sale_price / total_area, 0)) 
philly.sf_New <- philly.sf_New[!is.na(as.numeric(philly.sf_New$pricepersq)), ]
philly.sf_New <- st_join(philly.sf_New, nhoods_f) # Crime Data, School Data, Green Area Data, Shooting Incident Data, TOD Data
philly.sf_New <- st_join(philly.sf_New, philly.police) # Police District Data
philly.sf_New <- st_join(philly.sf_New, neighborhoods) # Neighborhood Data
philly.sf_New <- philly.sf_New %>%
  filter(name != "East Park" & name != "Wissahickon Park" & name != "Chinatown" & name != "Mechanicsville" & name != "University City" & name != "Pennypack Park")

# Price as a function of continuous variables
philly.sf_New %>%
  st_drop_geometry() %>%
  mutate(Age = 2024 - year_built) %>%
  dplyr::select(sale_price, total_area, Age, shooting_count, PPR_count, school_count, crime_count) %>%
  filter(sale_price <= 5000000, total_area <= 10000, 
         Age < 500) %>% # Filter out crazy outliers
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + 
     geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, ncol = 3, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     theme_minimal()
```

The following plots represents the difference in sale price by race, income, and whether it is a TOD area.
The mean sale price in majority white was significantly higher than that in majority non-white, and the mean sale price in high-income areas was significantly higher than that in low-income areas.
Meanwhile, the difference between TOD and non-TOD areas was not large, but the mean sale price in TOD areas was slightly higher.

```{r proposed_9, warning = FALSE, message = FALSE}
# Price as a function of categorical variables

philly.sf_New <- philly.sf_New %>%
  mutate(
    quality_grade_new = case_when(
      quality_grade == "1 " ~ "S ",  
      quality_grade == "2 " ~ "A ",  
      quality_grade == "3 " ~ "B ",  
      quality_grade == "4 " ~ "C ", 
      quality_grade == "5 " ~ "D ", 
      quality_grade == "6 " ~ "E ", 
      TRUE ~ as.character(quality_grade)        
    )
  )

philly.sf_New$quality_grade_new <- factor(philly.sf_New$quality_grade_new, levels = c("S+", "S ", "A+", "A ", "A-", "B+", "B ", "B-", "C+", "C ", "C-", "D+", "D ", "D-", "E+", "E ", "E-"))
# levels(philly.sf_filtered$quality_grade_new)
philly.sf_New %>%
  st_drop_geometry() %>%
  dplyr::select(sale_price, incomeContext, raceContext, TOD) %>%
  filter(sale_price <= 5000000, !is.na(incomeContext), !is.na(raceContext)) %>% # Filter out crazy outliers
  gather(Variable, Value, -sale_price) %>%
  ggplot(aes(Value, sale_price)) +
    geom_bar(position = "dodge", stat="summary", fun="mean")+
    facet_wrap(~Variable, ncol = 3, scales = "free") +
    labs(title = "Price as a function of categorical variables", y="Mean_Price") +
    plotTheme() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))+
    theme_minimal()
```

The following chart is correlation plot that shows the relationship between numeric variables.

```{r cormatrix, warning = FALSE, message = FALSE}
numericVars <- 
  select_if(st_drop_geometry(philly.sf_New), is.numeric) %>% na.omit()

ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation across numeric variables") 
```

The following regression denotes that `sale_price`, `exterior_condition`, `interior_condition`, `number_of_bathrooms`, `number_of_bedrooms`, `total_area`, `total_livable_area`, `year_built`, `number_stories` `shooting_count`, `crime_count`, and `TOD` were found to be correlated with `sale_price`, while `type_heater`,`view_type`, and `PPR_count` showed no clear evidence of correlation with `sale price`.
Because there were many missing values in `school_count`, regression values could not be obtained.

```{r proposed_10, warning = FALSE, message = FALSE}
philly.sf_New$school_count <- ifelse(is.na(philly.sf_New$school_count), 0, philly.sf_New$school_count)
# is.na(philly.sf_New$school_count)
reg1 = lm(sale_price ~ ., data = 
            st_drop_geometry(philly.sf_New) %>%
            dplyr::select(sale_price, exterior_condition, interior_condition, 
                          number_of_bathrooms, number_of_bedrooms,
                          total_area, total_livable_area, type_heater, 
                          view_type, year_built, number_stories, shooting_count, PPR_count, school_count, crime_count, TOD))
summary(reg1)
```

We divided `number_stories` into three groups (Up to 2 Floors, 3 Floors, 4+ Floors) and performed regression analysis.
The results are as follows.

```{r proposed_11, warning = FALSE, message = FALSE}
# Categorize Number of Stories
philly.sf_New <-
  philly.sf_New %>%
  mutate(number_stories.cat = case_when(
    number_stories >= 0 & number_stories < 3 ~ "Up to 2 Floors",
    number_stories >= 3 & number_stories < 4 ~ "3 Floors",
    number_stories > 4 ~ "4+ Floors"
  ))
```

```{r proposed_12, warning = FALSE, message = FALSE}
reg2 <- lm(sale_price ~ ., data = 
            st_drop_geometry(philly.sf_New) %>%
            dplyr::select(sale_price, exterior_condition, interior_condition, 
                          number_of_bathrooms, number_of_bedrooms,
                          total_area, total_livable_area, type_heater, 
                          view_type,  year_built, number_stories.cat, shooting_count, PPR_count, school_count, crime_count, TOD))
summary(reg2)
```

## Accuracy of the Proposed Model

As the same method in Chapter 2, we split our dataset into training and testing sets to evaluate our model's performance.
We used a 60/40 split for training and testing respectively.

The result of the proposed model is as follows:\
- Mean Absolute Error (MAE): \$86,664.03\
- Mean Absolute Percent Error (MAPE): 53.3%\
\* The result of knit can be different.

M In Chapter 2, MAE of the current model was \$100,278 and MAPE of the current model was 63.5%.
We can say that the proposed model is more accurate than the current model.

```{r proposed_13, warning = FALSE, message = FALSE}
inTrain <- createDataPartition(
              y = paste(philly.sf_New$number_stories.cat, 
                        philly.sf_New$view_type, philly.sf_New$heat_type), 
              p = .60, list = FALSE)

philly.training <- philly.sf_New[inTrain,] 
philly.test <- philly.sf_New[-inTrain,]  
```

```{r proposed_14, warning = FALSE, message = FALSE}
reg.training <- 
  lm(sale_price ~ ., data = as.data.frame(philly.training) %>% 
                             dplyr::select(sale_price, exterior_condition, interior_condition, 
                                           number_of_bathrooms, number_of_bedrooms,
                                           total_area, total_livable_area, type_heater, 
                                           view_type, year_built, number_stories.cat, shooting_count, PPR_count, school_count, crime_count, TOD))

summary(reg.training)

```

```{r proposed_15, warning = FALSE, message = FALSE}
philly.test <-
  philly.test %>%
  mutate(Regression = "Baseline Regression",
         sale_price.Predict = predict(reg.training, philly.test),
         sale_price.Error = sale_price.Predict - sale_price,
         sale_price.AbsError = abs(sale_price.Predict - sale_price),
         sale_price.APE = (abs(sale_price.Predict - sale_price)) / sale_price) %>%
  filter(sale_price < 5000000) 

ggplot()+
  geom_histogram(data = philly.test, aes(x = sale_price.AbsError), binwidth = 10000, fill = "orange") +
  scale_x_continuous(breaks=seq(0, 1000000, by = 100000), )+
  coord_cartesian(xlim = c(0, 1000000)) +
  labs(title = "Distribution of Prediction Errors",
  subtitle = "",
  caption = "") +
  theme_minimal()
```

```{r proposed_16, warning = FALSE, message = FALSE}
#Mean Absolute Error (MAE)
mean(philly.test$sale_price.AbsError, na.rm = T)
```

```{r proposed_17, warning = FALSE, message = FALSE}
# Mean Absolute Percent Error (MAPE)
mean(philly.test$sale_price.APE, na.rm = T)
```

The following plot shows that the model and perfect prediction.
The orange line represents a perfect prediction (y=x), and the green line represents average prediction.
This plot shows that predicted sale price with proposed model was still a little over estimated.

```{r proposed_18, warning = FALSE, message = FALSE}
ggplot(data=philly.test, aes(sale_price.Predict, sale_price)) +
     geom_point(size = .5) + 
     geom_abline(intercept = 0, slope = 1, color = "#FA7800") +
     geom_smooth(method = "lm", se=F, colour = "#25CB10") +
    coord_cartesian(xlim = c(0, 4000000), ylim = c(0, 4000000)) +
     labs(title = "Price as a function of continuous variables") +
     theme_minimal()
```

## Generalizability - 'k-fold' Cross-Validation

Compared to the current prediction model, the predicted sale price is slightly higher and the error is reduced.
However, the geographical distribution of moods is almost the same.

```{r cv2, warning = FALSE, message = FALSE}
fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(sale_price ~ ., data = st_drop_geometry(philly.sf_New) %>% 
                                dplyr::select(sale_price, exterior_condition, interior_condition, 
                                           number_of_bathrooms, number_of_bedrooms,
                                           total_area, total_livable_area, type_heater, 
                                           view_type,  year_built, number_stories.cat, shooting_count, PPR_count, school_count, crime_count, TOD), 
     method = "lm", trControl = fitControl, na.action = na.pass)

reg.cv

```

```{r distMAE2, warning = FALSE, message = FALSE}
ggplot()+
  geom_histogram(data = reg.cv$resample, aes(x = MAE), binwidth = 10000, fill = "orange") +
  scale_x_continuous(breaks=seq(0, 600000, by = 100000), )+
  coord_cartesian(xlim = c(0, 600000)) +
  labs(title = "Distribution of MAE",
  subtitle = "k-fold cross-validation; k=100",
  caption = "") +
  theme_minimal()
```

```{r predict_mapping2, warning = FALSE, message = FALSE}
philly.test <- philly.test[!is.na(philly.test$sale_price.Predict),]
ggplot() + 
  geom_sf(data=nhoods, fill="#DDD", color="white")+ 
  geom_sf(data = philly.test, aes(color = q5(sale_price.Predict)),  
          show.legend = "point", size = 0.5, alpha = 1) +  
  scale_color_manual(values = palette5,
                     labels=qBr(philly.test, "sale_price.Predict"),
                     name="Quintile\nBreaks($)") +
  labs(
    title = "Predicted Sale Price in Philadelphia",
    subtitle = "Assessment date: May 24, 2022-August 14, 2023",
    caption = "Data: U.S. Census Bureau, Zillow") +
  theme_void()
```

```{r error_mapping2, warning = FALSE, message = FALSE}
ggplot() + 
  geom_sf(data=nhoods, fill="#DDD", color="white")+ 
  geom_sf(data = philly.test, aes(color = q5(sale_price.AbsError)), 
          show.legend = "point", size = 0.5, alpha = 1) +  
  scale_color_manual(values = palette5,
                     labels=qBr(philly.test, "sale_price.AbsError"),
                     name="Quintile\nBreaks($)") + 
  labs(
    title = "Test Set Absolute Sale Price Errors in Philadelphia",
    subtitle = "Assessment date: May 24, 2022-August 14, 2023",
    caption = "Data: U.S. Census Bureau, Zillow") +
  theme_void()
```

## Check if Prices and Errors Cluster

### (1) Spatial Lags

Even in the proposed model, the errors are still clustered and spatial lag of error and sale price error are positively correlated.

```{r spatiallag_set3, warning = FALSE, message = FALSE}
coords <- st_coordinates(philly.sf_New) 

neighborList <- knn2nb(knearneigh(coords, 5))

spatialWeights <- nb2listw(neighborList, style="W")

philly.sf_New$lagPrice <- lag.listw(spatialWeights, philly.sf_New$sale_price)

```

```{r spatiallag_set4, warning = FALSE, message = FALSE}
philly.test <- philly.test[!is.na(philly.test$sale_price.Error),]
coords.test <-  st_coordinates(philly.test) 
neighborList.test <- knn2nb(knearneigh(coords.test, 5))
spatialWeights.test <- nb2listw(neighborList.test, style="W") 
```

```{r spatiallag_price2, warning = FALSE, message = FALSE}
# Sale Price as a function of the Spatial Lag of Price
philly.test %>% 
  mutate(lagPrice = lag.listw(spatialWeights.test, sale_price)) %>% 
  ggplot(aes(lagPrice, sale_price))+
  geom_point(color = "orange") +
  geom_smooth(method = "lm", se=F, colour = "#25CB10") +
  labs(title = "Price as a function of the Spatial Lag of Price",
       x = "Spatial Lag of Price (mean price of 5 nearest neighbors)",
       y = "Sale Price") +
  theme_minimal()
```

```{r spatiallag_error2, warning = FALSE, message = FALSE}
# Error as a Function of the Spatial Lag of Price
philly.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)) %>% 
  ggplot(aes(lagPriceError, sale_price.Error))+
  geom_point(color = "orange") +
  geom_smooth(method = "lm", se=F, colour = "#25CB10") +
  labs(title = "Error as a Function of the Spatial Lag of Price",
       x = "Spatial Lag of Errors (mean error of 5 nearest neighbors)",
       y = "Sale Price Error") +
  theme_minimal()
```

We can calculate the Pearson's R coefficient to test this a different way

```{r pearson, warning = FALSE, message = FALSE}

pearsons_test <- 
  philly.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error, NAOK=TRUE))

cor.test(pearsons_test$lagPriceError,
         pearsons_test$sale_price.Error, 
         method = "pearson")

```

### (2) Moran's I

We can assess the Moran's I value is 0.359 to see if the errors are spatially autocorrelated.
Moran's I in the proposed model became lower than the current model (0.395), but still indicates positive spatial autocorrelation.
This means that the errors are clustered, and the model is not capturing all the spatial patterns in the data.

```{r moransi2, warning = FALSE, message = FALSE}

moranTest <- moran.mc(philly.test$sale_price.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "orange",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  theme_minimal()
```

## Predictions by neighborhood

To validate the generalizability of the proposed model, we summarize the predictions by 158 neighborhood (`name` in the dataset).
This model examines the impact of neighborhood characteristics based on 158 neighborhoods.
The following table shows that the mean prediction and actual price of 158 neighborhoods.

```{r prediction_by_neighborhood, warning = FALSE, message = FALSE}


philly.test %>%
as.data.frame() %>%
  group_by(name) %>%
    summarize(meanPrediction = mean(sale_price.Predict),
              meanPrice = mean(sale_price)) %>%
      pander(caption = "Mean Predicted and Actual Sale Price by Neighborhood")

```

```{r reg1_by_neighborhood, warning = FALSE, message = FALSE}
reg.nhood <- lm(sale_price ~ ., data = as.data.frame(philly.training) %>% 
                                 dplyr::select(div_code, sale_price, exterior_condition, interior_condition, 
                                           number_of_bathrooms, number_of_bedrooms,
                                           total_area, total_livable_area, type_heater, 
                                           view_type,year_built, number_stories.cat, shooting_count, PPR_count, school_count, crime_count, TOD, name))
```

```{r reg2_by_neighborhood, warning = FALSE, message = FALSE}
summary(reg.nhood)

```

```{r test_by_neighborhood, warning = FALSE, message = FALSE}
philly.test.nhood <-
  philly.test %>%
  mutate(Regression = "Neighborhood Effects",
         sale_price.Predict = predict(reg.nhood, philly.test),
         sale_price.Error = sale_price.Predict- sale_price,
         sale_price.AbsError = abs(sale_price.Predict- sale_price),
         sale_price.APE = (abs(sale_price.Predict- sale_price)) / sale_price)%>%
  filter(sale_price < 5000000)
```

```{r reg_by_neighborhood, warning = FALSE, message = FALSE}
bothRegressions <- 
  rbind(
    dplyr::select(philly.test, starts_with("sale_price"), Regression, name) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)),
    dplyr::select(philly.test.nhood, starts_with("sale_price"), Regression, name) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)))  
```

The MAE and MAPE of the proposed model, which accounts for the neighborhood effect, are as follows: when incorporating the neighborhood effect, the MAE decreases by 20,000 and the MAPE decreases by 18%p compared to the proposed model without the neighborhood effect.

-   Initially, the neighborhood effect was defined by police districts, but upon resubmission, it was adjusted to 158 neighborhoods, and the neighborhood effect was recalculated.

```{r error_by_neighborhood, warning = FALSE, message = FALSE}
st_drop_geometry(bothRegressions) %>%
  gather(Variable, Value, -Regression, -name) %>%
  filter(Variable == "sale_price.AbsError" | Variable == "sale_price.APE") %>%
  group_by(Regression, Variable) %>%
    summarize(meanValue = mean(Value)) %>%
    spread(Variable, meanValue) %>%
    pander(caption = "Mean Absolute Error and Absolute Percentage Error by Regression")
```

```{r predicted_price_by_neighborhood, warning = FALSE, message = FALSE}
bothRegressions %>%
  dplyr::select(sale_price.Predict, sale_price, Regression) %>%
    ggplot(aes(sale_price, sale_price.Predict)) +
  geom_point() +
  stat_smooth(aes(sale_price, sale_price), 
             method = "lm", se = FALSE, size = 1, colour="orange") + 
  stat_smooth(aes(sale_price.Predict, sale_price), 
              method = "lm", se = FALSE, size = 1, colour="olivedrab") +
  facet_wrap(~Regression) +
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  theme_minimal()
```

### Spatial Evaluation of Errors

The following results of table and graph denote there is significant difference in MAE and MAPE by regression methods, particularly in Central and North Philadelphia.

```{r MAPE_by_neighborhood, warning = FALSE, message = FALSE}
st_drop_geometry(bothRegressions) %>%
  group_by(Regression, name) %>%
  summarize(mean.MAPE = mean(sale_price.APE, na.rm = T)) %>%
  ungroup() %>% 
  left_join(neighborhoods, by = c("name" = "name")) %>%
    st_sf() %>%
    ggplot() + 
      geom_sf(aes(fill = 100*mean.MAPE), color = "transparent") +
      geom_sf(data = bothRegressions, colour = "gray30", size = .5) +
      facet_wrap(~Regression) +
      scale_fill_gradient(low = palette5[1], high = palette5[5],
                          name = "MAPE") +
      labs(title = "Mean test set MAPE by neighborhood (Neighborhood)") +
      theme_void()

```

The following scatterplot shows the average MAPE by 158 neighborhood mean price.
The overall MAPE values were lower after accounting for the neighborhood effect, and the peak MAPE values observed before considering the neighborhood effect disappeared (Fairhill and McGuire).
In both regression methods, MAPE did show an decrease depending on the sale price.
However, in both methods, MAPE has a positive value.

```{r MAPE_by_neighborhood2, warning = FALSE, message = FALSE}
a <- st_drop_geometry(bothRegressions) %>%
  group_by(Regression, name) %>%
  summarize(mean.sale_price = mean(sale_price, na.rm = T))

a2 <- st_drop_geometry(bothRegressions) %>%
  group_by(Regression, name) %>%
  summarize(mean.MAPE = mean(sale_price.APE, na.rm = T))

a3<- left_join(a, a2, by = c("name" = "name", "Regression" = "Regression")) 

ggplot(a3) + 
    geom_point(aes(x = mean.sale_price, y = 100*mean.MAPE), color = "gray30", size = 1) +
    facet_wrap(~Regression) +
    scale_fill_gradient(low = palette5[1], high = palette5[5],
                        name = "MAPE") +
    labs(title = "Average MAPE by Neighborhood (Neighborhood) Mean Price") +
    theme_minimal()
```

### Race and Income Context of Predictions

We validated if the proposed model that accounts for neighborhood effect is applied to different group contexts: race and income.

In race, the regression that considered neighborhood effects resulted in a smaller MAPE in the majority white group.
In income, the regression that considered neighborhood effects resulted in a smaller MAPE in the high-income group.

For the majority non-White and low-income groups, the MAPE decreased significantly—by more than 30%p and 25%p, respectively—when the neighborhood effect was considered compared to when it was not.
It means the 'neighborhood effect' played an important role in predicting house prices.

As a result of generalizability, however, it can be concluded that it is relatively difficult to apply to the Majority Non-White or Low Income group.
We should consider other additional variables that may resolve the correlation between the groups or analyze more data.

```{r contextMapping, warning = FALSE, message = FALSE}
grid.arrange(ncol = 2,
  ggplot() + geom_sf(data = na.omit(nhoods), aes(fill = raceContext), color = "#999") +
    scale_fill_manual(values = c("olivedrab", "orange"), name="Race Context") +
    labs(title = "Race Context") +
    theme_void() + theme(legend.position="bottom"), 
  ggplot() + geom_sf(data = na.omit(nhoods), aes(fill = incomeContext), color = "#999") +
    scale_fill_manual(values = c("olivedrab", "orange"), name="Income Context") +
    labs(title = "Income Context") +
    theme_void() + theme(legend.position="bottom"))
```

```{r raceContextComparison, warning = FALSE, message = FALSE}
st_join(bothRegressions, nhoods_f) %>% 
  filter(!is.na(raceContext)) %>%
  group_by(Regression, raceContext) %>%
  summarize(mean.MAPE = 100*(mean(sale_price.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(raceContext, mean.MAPE) %>%
  pander(caption = "Test set MAPE by neighborhood race context(%)")
```

```{r incomeContextComparison, warning = FALSE, message = FALSE}
st_join(bothRegressions, nhoods_f) %>% 
  filter(!is.na(incomeContext)) %>%
  group_by(Regression, incomeContext) %>%
  summarize(mean.MAPE = 100*(mean(sale_price.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(incomeContext, mean.MAPE) %>%
  pander(caption = "Test set MAPE by neighborhood income context(%)")
```

# Conclusion and Discussion

We recommend that Zillow use our proposed OLS model instead of the current one.
Our proposed model aimed to predict housing prices using key features such as property condition, total livable area, number of bedrooms and bathrooms, proximity to crime, and access to amenities like schools and parks.
These variables are crucial as they directly influence property values based on what homebuyers prioritize.
The current Zillow model mainly focuses on internal characteristics and does not focus on public services or (dis)amenities.
However, our proposed model was able to obtain significantly reduced MAE and MAPE values by incorporating six public services, including crime, shooting, and police districts, which are actual factors that people around Philadelphia often consider when choosing their home location.
Our proposed model finally did effectively account for neighborhood effects, possibly because the neighborhood defined by 158 neighborhoods significantly decreased the MAE and MAPE.

While the model captured a significant portion of price variation, it was not perfect.
First, metrics like MAE and MAPE decreased compared to the current Zillow model, but there was still a positive correlation.
We also observed positive spatial autocorrelation in the model errors, as indicated by Moran's I statistics.
We need to identify more variables with spatial correlation.

Second, regression analysis did not work well for school data because the number of schools per tract had a lot of NA data.
In the future, it is necessary to reanalyze the distance to the nearest school from each house as continuous data.

Third, our proposed model should still improve generalizability.
The overall MAPE values were lower after accounting for the neighborhood effect, but MAPE by different group context has a positive value and large difference between two different groups which was race and house income, respectively.
The model showed better performance in high-income and predominantly white neighborhoods, evidenced by lower MAE values, possibly due to more stable socio-economic factors in these areas.
To improve generalization performance by eliminating these effects, we need to add sufficient other variables that can predict the variability occurring in low-income and major non-white groups, or change the weights of each variable.

In sum, further refinement is necessary to address local neighborhood variations that are not fully captured by the current set of predictors.
Incorporating more granular, demographically aligned, or culturally relevant boundaries could improve the model’s predictive power, allowing it to capture neighborhood effects more accurately and potentially reduce the overall error rate.

```{r}
philly.sf_Chall <- philly.sf %>%
  filter(sale_price < 5000000 & toPredict == "CHALLENGE") %>%
  mutate(pricepersq = ifelse(total_area > 10, sale_price / total_area, 0)) 
philly.sf_Chall <- philly.sf_Chall[!is.na(as.numeric(philly.sf_Chall$pricepersq)), ]
philly.sf_Chall <- st_join(philly.sf_Chall, nhoods_f) # Crime Data, School Data, Green Area Data, Shooting Incident Data, TOD Data
philly.sf_Chall <- st_join(philly.sf_Chall, philly.police) # Police District Data

philly.sf_Chall$sale_price <-  798332.8947 + philly.sf_Chall$exterior_condition * -27278.6103    +philly.sf_Chall$interior_condition* -27822.5115   +philly.sf_Chall$number_of_bathrooms*   60246.0186    +philly.sf_Chall$number_of_bedrooms*   -25245.0484    +philly.sf_Chall$total_livable_area*  193.4259       +philly.sf_Chall$year_built * -161.9947      +philly.sf_Chall$shooting_count * -1020.9659      +philly.sf_Chall$crime_count  * -18.7429 + ifelse(philly.sf_Chall$type_heater == "A", 13823.6306, 
ifelse(philly.sf_Chall$type_heater == "B", -8972.2810, 
ifelse(philly.sf_Chall$type_heater == "D", 239670.7969, 
ifelse(philly.sf_Chall$type_heater == "E", 58203.8361,
ifelse(philly.sf_Chall$type_heater == "H", 7760.8603,0)))))+
ifelse(philly.sf_Chall$view_type == "0", -82474.3969,
ifelse(philly.sf_Chall$view_type == "C", 182468.1793,
ifelse(philly.sf_Chall$view_type == "E", -50004.9183,0)))
#+ifelse(philly.sf_Chall$number_stories.cat =="4+ Floors",730714.0040,
#ifelse(philly.sf_Chall$number_stories.cat =="Up to 2 Floors",-45798.0893,0))

MUSA <- philly.sf_Chall %>%
  st_drop_geometry() %>%
  dplyr::select(musaID, sale_price) %>%
  mutate(team = "Agarwal and Jun") 
write.csv(MUSA,"mean_prediction_AgarwalJun.csv", row.names = FALSE)
```
